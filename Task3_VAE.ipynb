{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement autoencoder (AE), variational autoencoder (VAE), conditional variational autoencoder (CVAE) and a simple feed-forward neural network (NN).\n",
    "You will train these models on MNIST dataset, compare the results and make some conclusions about the comparison and about each model separately.\n",
    "The conclusions are mandatory.\n",
    "\n",
    "The required theory is below.\n",
    "The literature list is also available for those who want to further extend their knowledge about VAE.\n",
    "\n",
    "In this assignment the use of torch.distributions, torch.logsumexp and other standard libraries with distributions density, reparametrization trick and KL-divergence is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setting\n",
    "\n",
    "A set of independent and identically distributed samples from true data distribtuion is given: $x_i \\sim p_d(x)$, $i = 1, \\dots, N$.\n",
    "\n",
    "The problem is to build a probabilistic model $p_\\theta(x)$ of the true data distribution $p_d(x)$.\n",
    "\n",
    "The model $p_\\theta(x)$ must be able to estimate probabilistic density function (p. d. f.) for given $x$ and to sample $x \\sim p_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model\n",
    "$z \\in \\mathbb{R}^d$ is a latent variable.\n",
    "\n",
    "The generative process of VAE:\n",
    "1. Sample $z \\sim p(z)$.\n",
    "2. Sample $x \\sim p_\\theta(x | z)$.\n",
    "\n",
    "The parameters of distribution $p_\\theta(x | z)$ are obtained using a neural network with weights $\\theta$ and $z$ as an input.\n",
    "This network is called generator or decoder.\n",
    "\n",
    "The above generative process induce the following model p. d. f. for $x$:\n",
    "\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameterization\n",
    "\n",
    "A priori distribution on the latent varibales is standard normal distribution: $p(z) = \\mathcal{N}(z | 0, I)$.\n",
    "\n",
    "The distributions on the components of $x$ are conditionally independent given $z$: $p_\\theta(x | z) = \\prod\\limits_{i = 1}^D p_\\theta(x_i | z)$.\n",
    "\n",
    "If i-th component is real-valued, we can use Gaussian generative distribution: $p_\\theta(x_i | z) = \\mathcal{N}(x_i | \\mu_i(z, \\theta), \\sigma^2_i(z, \\theta))$.\n",
    "Here $\\mu(z, \\theta)$ Ð¸ $\\sigma(z, \\theta)$ are deterministic functions defined by neural networks with parameters $\\theta$.\n",
    "\n",
    "If i-th component is categorial, then we can use categorical generative distribution: $p_\\theta(x_i | z) = Cat(Softmax(\\omega_i(z, \\theta)))$, where $\\omega_i(z, \\theta)$ is also a deterministic function described by neural network.\n",
    "\n",
    "Binary components are the special case of categorical ones. For them categorical distribution turns into Bernoulli distibution with just one parameter.\n",
    "\n",
    "_Tip:_ some pixels are black in the whole MNIST train set, so likelihood maximization forces the probability of these pixels to be black to 1.\n",
    "Therefore the weights for these pixels tend to become closer to infinity.\n",
    "To avoid divergence of the training procedure, we may add a clipping level into generative network: e. g. clipping layer into range $[-10, 10]$ before final activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational lower bound\n",
    "\n",
    "To fit the model to data we maximize marginal log-likelihood $\\log p_\\theta(x)$ of the train set.\n",
    "\n",
    "Nevertheless, $\\log p_\\theta(x)$ cannot be optimized straightforwardly, because there is integral in high-dimensional space inside the logarithm which cannot be computed analytically or numerically estimated with enough accuracy in a reasonable amount of time.\n",
    "\n",
    "So to perform optimization we maximize the _variational lower bound_ (VLB) on log-likelihood instead:\n",
    "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x) = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z) q_\\phi(z | x)}{q_\\phi(z | x) p_\\theta(z | x)} = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} + KL(q_\\phi(z | x) || p_\\theta(z | x))$$\n",
    "$$\\log p_\\theta(x) \\geqslant \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x | z)p(z)}{q_\\phi(z | x)} = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z) - KL(q_\\phi(z | x) || p(z)) = L(x; \\phi, \\theta)\n",
    "\\to \\max\\limits_{\\phi, \\theta}$$\n",
    "\n",
    "$q_\\phi(z | x)$ is called a proposal, recognition or variational distribution. It is ususally defined as a Gaussian with parameters from a neural network with weights $\\phi$ which takes $x$ as an input:\n",
    "$q_\\phi(z | x) = \\mathcal{N}(z | \\mu_\\phi(x), \\sigma^2_\\phi(x)I)$.\n",
    "Usually neural network defines $\\log\\sigma_\\phi(x)$ or $\\log(\\exp(\\sigma_\\phi(x) - 1))$ instead of $\\sigma_\\phi(x)$. So $\\sigma_\\phi(x)$ is always positive by design and also more scale-independent. We use $\\log(\\exp(\\sigma_\\phi(x) - 1))$ in this assignment because its reverse transition is Softplus and behaves more stable for big positive preactivations than Exp.\n",
    "\n",
    "#### Discussion of VLB\n",
    "\n",
    "One can show that the gap between VLB $L(x; \\phi, \\theta)$ on log-likelihood and the log-likelihood $\\log p_\\theta(x)$ itself is KL-divergence between proposal and aposteriori distributions over $z$: $KL(q_\\phi(z | x) || p_\\theta(z | x))$.\n",
    "Maximum of $L(x; \\phi, \\theta)$ with fixed $\\theta$ is achieved when $q_\\phi(z | x) = p_\\theta(z | x)$.\n",
    "Nevertheless, $p_\\theta(z | x)$ is untractable, so instead of numerically computing it, VLB is optimized w. r. t. $\\phi$ using backpropagation and reparameterization trick (see below).\n",
    "The closer $q_\\phi(z | x)$ to $p_\\theta(z | x)$, the more precise is VLB.\n",
    "The true posterior distribution $p_\\theta(z | x)$ often cannot be decribed by one Gaussian, so the gap between VLB and LL never reaches zero.\n",
    "\n",
    "The first term of VLB - $\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z)$ - is called reconstruction loss.\n",
    "The model describes this term is an autoencoder with one stochastic layer which tries to restore input object $x$.\n",
    "If $q_\\phi(z | x)$ is a delta-function, then an aoutoencoder with a stochastic layer turns into an ordinary autoencoder.\n",
    "That is why $q_\\phi(z | x)$ and $p_\\theta(x | z)$ are called encoder and decoder respectivelly.\n",
    "\n",
    "The term $KL(q_\\phi(z | x) || p(z))$ is called regularizer.\n",
    "It forces $z \\sim q_\\phi(z | x)$ to be close to $0$.\n",
    "But, as described above, it also forces $q_\\phi(z | x)$ to be close to $p_\\theta(z | x)$, which is even more important.\n",
    "Somebody use a coefficient before KL-divergence or even a different regularizer.\n",
    "Naturally, after that optimization of VLB usually becomes unrelated to the log-likelihood of the initial probabilistic model.\n",
    "This decreases intrpretability of the model and avoids theoretical guarantees.\n",
    "\n",
    "KL-divergence between two Gaussians can be computed analytically, which improves the speed and stability of optimization procedure.\n",
    "\n",
    "#### Reparameterization trick\n",
    "We use stochastic gradient ascent in order to maximize $L(x; \\phi, \\theta)$.\n",
    "\n",
    "The gradient of KL-divergence w. r. t. $\\phi$ can be derived analytically as well as KL-divergence itself in the case of two Gaussians.\n",
    "\n",
    "The gradient of the reconstruction loss w. r. t. $\\theta$ is computed using backpropagation.\n",
    "$$\\frac{\\partial}{\\partial \\theta} L(x; \\phi, \\theta) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{\\partial}{\\partial \\theta} \\log p_\\theta(x | z)$$\n",
    "\n",
    "The gradient of the reconstruction loss w. r. t. $\\phi$ can be computed using reparametrization trick:\n",
    "$$\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)$$\n",
    "$$z = \\mu + \\sigma \\varepsilon \\Rightarrow z \\sim \\mathcal{N}(z | \\mu, \\sigma^2I)$$\n",
    "$$\\frac{\\partial}{\\partial \\phi} L(x; \\phi, \\theta) = \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)} \\frac{\\partial}{\\partial \\phi} \\log p_\\theta(x | \\mu_\\phi(x) + \\sigma_\\phi(x) \\varepsilon) - \\frac{\\partial}{\\partial \\phi} KL(q_\\phi(z | x) || p(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tip:_ the main trick in deriving KL-divergence between two Gaussians is to never write the integral sign.\n",
    "We consider only Gaussians with diagonal covariation matrix, so it comes to derive KL-divergence between two one-dimensional Gaussians.\n",
    "All you need to derive the formula:\n",
    "$$KL(q || p) = \\mathbb{E}_{z \\sim q} \\log\\frac{q(z)}{p(z)}$$\n",
    "$$\\mathrm{N}(z | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(-\\frac{(z - \\mu)^2}{2\\sigma^2} \\right)$$\n",
    "$$\\mathbb{E}_{z \\sim N(\\mu, \\sigma)}z = \\mu$$\n",
    "$$\\mathbb{E}_{z \\sim N(\\mu, \\sigma)}z^2 = \\mu^2 + \\sigma ^ 2$$\n",
    "Btw, the above equations have multidimensional generalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood estimation\n",
    "\n",
    "Model log-likelihood $\\log p_\\theta(x) = \\log \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$ is estimated using the hold-out validation set.\n",
    "\n",
    "Likelihood can be estimated using Monte-Carlo method:\n",
    "\n",
    "$$z_i \\sim p(z), i = 1, \\dots, K$$\n",
    "$$p_\\theta(x) \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i)$$\n",
    "\n",
    "An alternative is importance sampling method.\n",
    "We use the proposal distribution from model as the proposal distribution.\n",
    "The good choice of the proposal distribution is known to decrease the estimate variance for importance sampling.\n",
    "\n",
    "For VAE, Monte-Carlo with a small amount of samples underestimates true log-likelihood.\n",
    "Imporatance sampling allows estimating likelihood for VAE more precisely with a small amount of samples.\n",
    "\n",
    "$$z_i \\sim q_\\phi(z | x), i = 1, \\dots, K$$\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{p_\\theta(x | z) p(z)}{q_\\phi(z | x)} \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)}$$\n",
    "\n",
    "The both above estimates are unbiased, but also useless for us.\n",
    "First, likelihood density in a high-dimensional space usually too small to be handled by floating point computer arithmetics.\n",
    "Second, even after likelihood estimation for one object we have to multiply these estimations to get the likelihood of the whole validation set.\n",
    "\n",
    "So instead of likelihood we estimate log-likelihood for each object which allows us to avoid numerical unstability.\n",
    "\n",
    "For log-likelihood estimation the averaging is also performed inside the logarithm:\n",
    "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i),\\,\\,\\,\\,z_i \\sim p(z)$$\n",
    "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)},\\,\\,\\,\\,z_i \\sim q_\\phi(z | x)$$\n",
    "Note that these etimates are not unbiased.\n",
    "Nevertheless, the first estimate is referred sometimes as Monte-Carlo log-likelihood estimate.\n",
    "The second estimate is known as IWAE estimate for the paper Importance Weighted Variational Autoencoders which proposes to directly optimize this estimate for learning VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Auto-Encoding Variational Bayes https://arxiv.org/pdf/1312.6114.pdf, Stochastic Backpropagation and Approximate Inference in Deep Generative Models https://arxiv.org/pdf/1401.4082.pdf - the original papers on VAE (two research groups independently and almost simultaneusly proposed the same model).\n",
    "2. Learning Structured Output Representation using Deep Conditional Generative Models https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf - conditional VAE for sampling from conditional distributions.\n",
    "3. Importance Weighted Autoencoders https://arxiv.org/pdf/1509.00519.pdf - variational autoencoder which optimizes more accurate lower bound on log-likelihood.\n",
    "4. Tighter Variational Bounds are Not Necessarily Better https://arxiv.org/pdf/1802.04537.pdf - the paper which shows that the tighter lower bound from the previous paper leads to the proposal networks underfitting and proposes the ways to solve the problem.\n",
    "5. Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives https://openreview.net/forum?id=HkG3e205K7 - the paper which proposes variance reduction optimization procedure which also solves the problem from the paper above.\n",
    "6. Variational Inference with Normalizing Flows https://arxiv.org/pdf/1505.05770.pdf, Improved Variational Inference with Inverse Autoregressive Flow http://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf - richer families of the proposal distributions based on flows.\n",
    "7. VAE with a VampPrior https://arxiv.org/pdf/1705.07120.pdf - learning the prior distribution on the latent variable together with the proposal one. Improves model log-likelihood but makes the latent space less interpretable.\n",
    "8. Ladder Variational Autoencoders http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf - now an object has not only one latent representation but a hierarchy of them.\n",
    "9. Inference Suboptimality in Variational Autoencoders https://arxiv.org/pdf/1801.03558.pdf - the paper states that the gap between VLB and log-likelihood is more due to the lack of proposal network capacity than due to the poor proposal distributions family. Nevertheless, richer proposal distributions family soften the requirements to the network capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "Let's do some practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST('mnist', download=True, train=True)\n",
    "train_data_raw = TensorDataset(data.train_data.view(-1, 28 * 28).float() / 255, data.train_labels)\n",
    "train_data = TensorDataset(train_data_raw.tensors[0].round())\n",
    "data = MNIST('mnist', download=True, train=False)\n",
    "test_data_raw = TensorDataset(data.test_data.view(-1, 28 * 28).float() / 255, data.test_labels)\n",
    "test_data = TensorDataset(test_data_raw.tensors[0].round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_images(x, scale=15, line_width=10):\n",
    "    plt.figure(figsize=(scale, scale / line_width * (x.shape[0] // line_width + 1)))\n",
    "    x = x.view(-1, 1, 28, 28)\n",
    "    mtx = torchvision.utils.make_grid(x, nrow=line_width, pad_value=1)\n",
    "    plt.imshow(mtx.permute([1, 2, 0]).numpy(), cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "    plt.axis('off')\n",
    "\n",
    "show_images(train_data[:10][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Must be using Python 3\")\n",
    "\n",
    "if sys.version_info[1] >= 7:\n",
    "    from contextlib import nullcontext\n",
    "else:\n",
    "    class nullcontext:\n",
    "        def __init__(self, enter_context=None):\n",
    "            self.enter_context = enter_context\n",
    "\n",
    "        def __enter__(self):\n",
    "            return self.enter_context\n",
    "\n",
    "        def __exit__(self, *args, **kwargs):\n",
    "            return None\n",
    "\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(compute_loss, data, batch_size=128, num_samples=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Computes averaged loss function over the test data.\n",
    "    Input: compute_loss, function of batch (type torch.FloatTensor),\n",
    "           returns single float - loss function on the batch.\n",
    "    Input: data, Dataset - for testing.\n",
    "    Input: batch_size, int.\n",
    "    Input: num_samples, int - if set, then stop computing\n",
    "           after processing num_samples objects. Otherwise\n",
    "           compute loss function over the whole test set.\n",
    "    Input: verbose, bool - whether to print the progress.\n",
    "    Return: float - the loss function estimate.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=(num_samples is None))\n",
    "    samples_processed = 0\n",
    "    avg_loss = 0\n",
    "    raw_iterator = enumerate(dataloader)\n",
    "    with (tqdm(raw_iterator, total=len(dataloader)) if verbose else nullcontext(raw_iterator)) as iterator:\n",
    "        for i, batch in iterator:\n",
    "            samples_processed += int(batch[0].shape[0])\n",
    "            if len(batch) == 1:\n",
    "                batch = batch[0]\n",
    "            loss = compute_loss(batch)\n",
    "            avg_loss += (loss - avg_loss) / (i + 1)\n",
    "            if verbose:\n",
    "                iterator.set_description('Test loss: %.3f' % avg_loss)\n",
    "            if num_samples and samples_processed >= num_samples:\n",
    "                break\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(parameters, batch_loss, data, tests=[], maximization=True,\n",
    "                tests_per_epoch=5, batch_size=64, num_epochs=5,\n",
    "                learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Perfrom stochastic gradient descent or ascent over batch_loss\n",
    "    function and stores the learning progress.\n",
    "    Input: batch_loss - fucntion of batch which returns\n",
    "           differentiabale loss scalar tensor.\n",
    "    Input: data, Dataset - for training.\n",
    "    Input: tests - a list of tests performed during the optimization.\n",
    "           Each list element is a dictionary with two fields:\n",
    "           'name' is an unique identifier of the test,\n",
    "           'func' is a test function with no parameters.\n",
    "    Input: maximization, bool - whether to maximize batch_loss or\n",
    "           to minimize.\n",
    "    Input: tests_per_epoch, int.\n",
    "    Input: batch_size, int.\n",
    "    Input: num_epochs, int.\n",
    "    Input: learning_rate, float.\n",
    "    Return: a dictionary with fileds\n",
    "            'train_losses_list' - a list of batch loss values on\n",
    "                                  every batch\n",
    "            'test_results' -      a dictionary of pairs\n",
    "                                  <test_name>: <list of the test results>\n",
    "            'num_epochs' -        number of epochs\n",
    "    \"\"\"\n",
    "    gd = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    test_results = {test['name']: [] for test in tests}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, timeout=0.5)\n",
    "        num_batches_between_tests = len(dataloader) // tests_per_epoch + 1\n",
    "        avg_train_loss = 0\n",
    "    \n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader)) as iterator:\n",
    "            for batch_num, batch in iterator:\n",
    "                if len(batch) == 1:\n",
    "                    batch = batch[0]\n",
    "                gd.zero_grad()\n",
    "                loss = batch_loss(batch)\n",
    "                if maximization:\n",
    "                    (-loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                gd.step()\n",
    "\n",
    "                train_losses.append(float(loss))            \n",
    "\n",
    "                avg_train_loss += (float(loss) - avg_train_loss) / (batch_num + 1)\n",
    "                iterator.set_description('Train loss: %.3f' % avg_train_loss)\n",
    "\n",
    "                if batch_num % num_batches_between_tests == 0 or \\\n",
    "                        (epoch == num_epochs - 1 and batch_num == len(iterator) - 1):\n",
    "                    for test in tests:\n",
    "                        test_results[test['name']].append(test['func']())\n",
    "    return {\n",
    "        'train_losses_list': train_losses,\n",
    "        'test_results': test_results,\n",
    "        'num_epochs': num_epochs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "digit_size = 28\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "def draw_manifold(generator):\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    for i, yi in enumerate(reversed(grid_x)):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "\n",
    "            x_decoded = generator(z_sample)\n",
    "            digit = x_decoded\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_latent_space(data, target, encoder):\n",
    "    z_test = encoder(data)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.scatter(z_test[:, 0], z_test[:, 1], c=target, cmap='gist_rainbow', alpha=0.75)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticks(num_points, num_epochs):\n",
    "    return np.arange(num_points) / (num_points - 1) * num_epochs\n",
    "\n",
    "def draw_line(points, num_epochs, label):\n",
    "    plt.plot(get_ticks(len(points), num_epochs), points, label=label)\n",
    "\n",
    "def smooth_line(points, chunk_size=10):\n",
    "    res = []\n",
    "    for i in range((len(points) - 1) // chunk_size + 1):\n",
    "        chunk = points[i * chunk_size: (i + 1) * chunk_size]\n",
    "        res.append(sum(chunk) / len(chunk))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        \"\"\"\n",
    "        Initialize model weights.\n",
    "        Input: d, int - the dimensionality of the latent space.\n",
    "        Input: D, int - the dimensionality of the object space.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.D, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.d)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.d, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.D),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Generate a latent code given the objects.\n",
    "        Input: x, Tensor of shape n x D.\n",
    "        Return: Tensor of shape n x d.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Generate objects given the latent representations.\n",
    "        Input: z, Tensor of shape n x d - the latent representations.\n",
    "        Return: Tensor of shape n x D.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Compute batch loss. Batch loss is an average of per-object losses.\n",
    "        Per-object loss is a sum of reconstruction L2-error and\n",
    "        L2-regularization of the latent representations.\n",
    "        Tip: do not average average per-pixel L2-errors for the object, use\n",
    "        their sum instead. Also do not average L2-reguralization for\n",
    "        the components in the latent space.\n",
    "        The returned scalar must be differentiable w. r. t. the weights\n",
    "        of the model. (!)\n",
    "        Input: batch, Tensor of shape n x D.\n",
    "        Return: Tensor, scalar - loss function for the batch.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generate samples from standard normal distribution in the latent space.\n",
    "        Input: num_samples, int - number of sample to be generated.\n",
    "        Return: Tensor of shape num_samples x D.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tests = lambda model: [\n",
    "    {\n",
    "        'name': 'test_loss',\n",
    "        'func': lambda: test_loss(lambda batch:\n",
    "                                  float(model.batch_loss(batch)),\n",
    "                                  test_data,\n",
    "                                  num_samples=1000, verbose=False)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_d2 = AE(2, 784)\n",
    "ae_d2_train_log = train_model(ae_d2.parameters(), ae_d2.batch_loss, train_data, tests=ae_tests(ae_d2), \n",
    "                            maximization=False, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_d10 = AE(10, 784)\n",
    "ae_d10_train_log = train_model(ae_d10.parameters(), ae_d10.batch_loss, train_data, tests=ae_tests(ae_d10),\n",
    "                             maximization=False, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(ae_d2.generate_samples(20).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(ae_d10.generate_samples(20).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space visualization (from the decoder's side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_manifold_ae(model):\n",
    "    generator = lambda z: model.decode(torch.from_numpy(z).float()).view(28, 28).data.numpy()\n",
    "    return draw_manifold(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_manifold_ae(ae_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space visualization (from the encoder's side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latent_space(test_data_raw.tensors[0][::10], test_data_raw.tensors[1][::10],\n",
    "                  lambda data: ae_d2.encode(data).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_encoder_d10 = lambda data: TSNE().fit_transform(ae_d10.encode(data).data.numpy())\n",
    "draw_latent_space(test_data_raw.tensors[0][::25], test_data_raw.tensors[1][::25], ae_encoder_d10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.title('Autoencoder training progress')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "draw_line(smooth_line(ae_d2_train_log['train_losses_list']), ae_d2_train_log['num_epochs'], '$d=2$, train loss')\n",
    "draw_line(ae_d2_train_log['test_results']['test_loss'], ae_d2_train_log['num_epochs'], '$d=2$, test loss')\n",
    "draw_line(smooth_line(ae_d10_train_log['train_losses_list']), ae_d10_train_log['num_epochs'], '$d=10$, train loss')\n",
    "draw_line(ae_d10_train_log['test_results']['test_loss'], ae_d10_train_log['num_epochs'], '$d=10$, test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_log_likelihood(x_true, x_distr):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood of objects x_true for the generated by model\n",
    "    component-wise Bernoulli distributions.\n",
    "    Each object from x_true has K corresponding distrbutions from x_distr.\n",
    "    Log-likelihood estimation must be computed for each pair of an object\n",
    "    and a corresponding to the object distribution.\n",
    "    Do not forget about computational stability!\n",
    "    Do not divide log-likelihood by the dimensionality of the space of objects.\n",
    "\n",
    "    Input: x_true, Tensor of shape n x D.\n",
    "    Input: x_distr, Tensor of shape n x K x D - parameters of component-wise\n",
    "           Bernoulli distributions.\n",
    "    Return: Tensor of shape n x K - log-likelihood for each pair of an object\n",
    "            and a corresponding distribution.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def kl(q_mu, q_sigma, p_mu, p_sigma):\n",
    "    \"\"\"\n",
    "    \n",
    "    Compute KL-divergence KL(q || p) between n pairs of Gaussians\n",
    "    with diagonal covariational matrices.\n",
    "    Do not divide KL-divergence by the dimensionality of the latent space.\n",
    "\n",
    "    Input: q_mu, p_mu, Tensor of shape n x d - mean vectors for n Gaussians.\n",
    "    Input: q_sigma, p_sigma, Tensor of shape n x d - standard deviation\n",
    "           vectors for n Gaussians.\n",
    "    Return: Tensor of shape n - each component is KL-divergence between\n",
    "            a corresponding pair of Gaussians.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClampLayer(nn.Module):\n",
    "    def __init__(self, min=None, max=None):\n",
    "        super().__init__()\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.kwargs = {}\n",
    "        if min is not None:\n",
    "            self.kwargs['min'] = min\n",
    "        if max is not None:\n",
    "            self.kwargs['max'] = max\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.clamp(input, **self.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        \"\"\"\n",
    "        Initialize model weights.\n",
    "        Input: d, int - the dimensionality of the latent space.\n",
    "        Input: D, int - the dimensionality of the object space.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.proposal_network = nn.Sequential(\n",
    "            nn.Linear(self.D, 200),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.proposal_mu_head = nn.Linear(200, self.d)\n",
    "        self.proposal_sigma_head = nn.Sequential(\n",
    "            nn.Linear(200, self.d),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.generative_network = nn.Sequential(\n",
    "            nn.Linear(self.d, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.D),\n",
    "            ClampLayer(-10, 10),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def proposal_distr(self, x):\n",
    "        \"\"\"\n",
    "        Generate proposal distribution over z.\n",
    "        Note that sigma is positive by design of neural network.\n",
    "        Input: x, Tensor of shape n x D.\n",
    "        Return: tuple(Tensor, Tensor),\n",
    "                Each Tensor is a matrix of shape n x d.\n",
    "                The first one is mu, the second one is sigma.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        return mu, sigma\n",
    "\n",
    "    def prior_distr(self, x):\n",
    "        \"\"\"\n",
    "        Generate prior distribution over z.\n",
    "        Note that sigma is positive by design of neural network.\n",
    "        Input: x, Tensor of shape n x D.\n",
    "        Return: tuple(Tensor, Tensor),\n",
    "                Each Tensor is a matrix of shape n x d.\n",
    "                The first one is mu, the second one is sigma.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_latent(self, mu, sigma, K=1):\n",
    "        \"\"\"\n",
    "        Generate samples from Gaussians with diagonal covariance matrices in latent space.\n",
    "        Samples must be differentiable w. r. t. parameters of distribution!\n",
    "        Use reparametrization trick.\n",
    "        Input: mu, Tensor of shape n x d - mean vectors for n Gaussians.\n",
    "        Input: sigma, Tensor of shape n x d - standard deviation vectors\n",
    "               for n Gaussians.\n",
    "        Input: K, int - number of samples from each Gaussian.\n",
    "        Return: Tensor of shape n x K x d.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        return sample\n",
    "\n",
    "    def generative_distr(self, z):\n",
    "        \"\"\"\n",
    "        Compute a tensor of parameters of Bernoulli distribution over x\n",
    "        given a tensor of latent representations.\n",
    "        Input: z, Tensor of shape n x K x d - tensor of latent representations.\n",
    "        Return: Tensor of shape n x K x D - parameters of Bernoulli distribution.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        return params\n",
    "\n",
    "    def batch_vlb(self, batch):\n",
    "        \"\"\"\n",
    "        Compute VLB for batch. The VLB for batch is an average of VLBs for batch's objects.\n",
    "        VLB must be differentiable w. r. t. model parameters, so use reparametrization!\n",
    "        Input: batch, Tensor of shape n x D.\n",
    "        Return: Tensor, scalar - VLB.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        return vlb\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "        Tip: for visual quality you may return the parameters of Bernoulli distribution instead\n",
    "        of samples from it.\n",
    "        Input: num_samples, int - number of samples to generate.\n",
    "        Return: Tensor of shape num_samples x D.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mean_exp(data):\n",
    "    \"\"\"\n",
    "    Return log(mean(exp(data))) where mean is taken over the last dimension.\n",
    "    Do not forget about computational stability!\n",
    "    Using torch.logsumexp is prohibited!\n",
    "    Input: data, Tensor of shape n_1 x n_2 x ... x n_K.\n",
    "    Return: Tensor of shape n_1 x n_2 x ,,, x n_{K - 1}.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    return res\n",
    "\n",
    "def gaussian_log_pdf(mu, sigma, samples):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood of samples for a given Gaussians with diagonal covariance matrices.\n",
    "    Input: mu, Tensor of shape n x d - mean vectors for n Gaussians.\n",
    "    Input: sigma, Tensor of shape n x d - standard deviation vectors for n Gaussians.\n",
    "    Input: samples, Tensor of shape n x K x d.\n",
    "    Return: Tensor of shape n x K - element (i, j) is log-likelihood of (i, j)-th sample\n",
    "            w. r. t. i-th Gaussian.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    return log_pdf\n",
    "\n",
    "\n",
    "def compute_log_likelihood_monte_carlo(batch, model, generative_log_likelihood, K):\n",
    "    \"\"\"\n",
    "    Monte-Carlo log-likelihood estimation for a batch.\n",
    "    Log-likelihood must be averaged over all objects in the batch.\n",
    "    Do not forget to convert the result into float! Otherwise the average of such results forms\n",
    "    the computational graph stored in memory, which naturaly results in memory overflow soon enough.\n",
    "    Input: batch, Tensor of shape n x D for VAE or pair of Tensors for CVAE\n",
    "    Input: model, Module - object with methods prior_distr, sample_latent and generative_distr,\n",
    "           described in VAE class.\n",
    "    Input: generative_log_likelihood, function which takes batch and distribution parameters\n",
    "           produced by the generative network.\n",
    "    Input: K, int - number of latent samples.\n",
    "    Return: float - average log-likelihood estimate for the batch.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_tests = lambda model: [\n",
    "    {\n",
    "        'name': 'MC',\n",
    "        'func': lambda:\n",
    "                test_loss(lambda batch:\n",
    "                                compute_log_likelihood_monte_carlo(batch, model, bernoulli_log_likelihood, K=10),\n",
    "                          test_data,\n",
    "                          num_samples=1000)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_d2 = VAE(2, 784)\n",
    "vae_d2_train_log = train_model(vae_d2.parameters(), vae_d2.batch_vlb, train_data,\n",
    "                               tests=vae_tests(vae_d2), num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_d10 = VAE(10, 784)\n",
    "vae_d10_train_log = train_model(vae_d10.parameters(), vae_d10.batch_vlb, train_data,\n",
    "                                tests=vae_tests(vae_d10), num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(vae_d2.generate_samples(20).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(vae_d10.generate_samples(20).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space visualization (from the decoder's side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_manifold_vae(model):\n",
    "    generator = lambda z: model.generative_distr(torch.from_numpy(z).unsqueeze(1).float()).view(28, 28).data.numpy()\n",
    "    return draw_manifold(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_manifold_vae(vae_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space visualization (from the encoder's side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder = lambda data, model: model.sample_latent(*model.proposal_distr(data))[:, 0].detach()\n",
    "draw_latent_space(test_data_raw.tensors[0][::10], test_data_raw.tensors[1][::10],\n",
    "                  lambda data: vae_encoder(data, vae_d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder_d10 = lambda data: TSNE().fit_transform(vae_encoder(data, vae_d10).data.numpy())\n",
    "draw_latent_space(test_data_raw.tensors[0][::25], test_data_raw.tensors[1][::25], vae_encoder_d10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The place for your conclusions\n",
    "What is the difference between Autoencoder and Variational Autoencoder?\n",
    "\n",
    "How does latent space dimensionality affect models?\n",
    "\n",
    "What else can you say about models, learning progress, samples, log-likelihood estimates, latent spaces and representations?\n",
    "\n",
    "Feel free to express your observations and hypoteses below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">YOUR TEXT HERE</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
